---
layout:     post
title:      "第4章 决策树"
date:       1994-11-05
author:     "Ziven"
header-img: "img/in-post/zzh-machine-learning/bg.jpg"
header-mask:  0.3
catalog:      true
tags:
    - 学习笔记
    - 机器学习
---

返回[目录](http://ziven.xin/2017/07/06/zzh-machine-learning-outline/)   
---------------------------------------------------------------
用于查看完整数学公式：手机用户`长按公式`/PC用户`右击公式`出现菜单栏点击`Math Settings`--->`Zoom Trigger`--->`Double-Click`即可`双击放大`文中基于mathjax的公式与图表  

## 1. 什么是决策树

在日常生活中，当我们判别一件商品的好坏时，经常会通过这类商品的某些属性来进行决策，例如判断一个西瓜是否是好瓜，我们通常会先观察这个西瓜的**色泽**是不是*青绿色*的，如果**色泽**是*青绿色*的则继续观察它的**根蒂**是不是*蜷缩*的，如果**根蒂**是*蜷缩*的，则会听听它的**敲声**是否*清脆*，如果满足以上一系列条件那么我们就将这个西瓜判别为很可能是个好瓜。以上的决策过程可以用下图的树形结构来表示，而上文中的粗体字即为样本的属性，斜体字即为对应熟悉的属性值，例如属性**色泽**有属性值：*青绿*、*乌黑*、*浅白*。       

![1532846357225](D:\Works\GitHub\zivenyang.github.io\img\in-post\zzh-machine-learning\ch4\什么是决策树.png)   

一般来讲，一颗决策树包含一个根结点，若干个内节点和若干个叶子结点，根结点中包含了**所有的样本**，而内结点中包含了所有满足从根结点到父结点中属性要求的**样本子集**，叶子结点即为满足这条分支上所有属性要求的样本所属的**类别**。       

决策树的目的是为了产生一颗泛化能力强的决策树，其基本流程如下：    

![img](file:///D:\Softwares\QQ\数据文档\865059488\Image\C2C\{B9F6D5EF-1440-1864-5974-17A7278613D8}.png)    

```python
# 伪代码
D = [(x1, y1), (x2, y2), ..., (xm, ym)]
A = {"a1": [a1v1, a1v2, ..., a1vn1],
     "a2": [a1v1, a1v2, ..., a1vn2],
     ... ... ... ... ... ... ... ..,
     "ad": [a1v1, a1v2, ..., a1vn3]}

def TreeGenerate(D, A):
	生成结点 node
    
    if D中样本全属于同一类别C:
        将node标记为C类叶结点
        return
    
    if A为空 or D中的样本在A上取值相同:
        将node标记为叶节点，其类别标记为D中样本数最多的类别
        return
    
    从A中选择最优划分属性a*
    for a*v in A["a*"]：
    	为node生成一个分支
        Dv=D[A["a*"]==a*v]
        if Dv is None:
            将分支结点标记为叶结点，其类别标记为D中样本最多的类被；
            return
        else:
            TreeGenerate(Dv, A-A["a*"])
            
```

递归终止条件：       

1. 若当前节点包含的样本全属于同一类别则无需划分；
2. 若当前节点的属性集为空，或者所有属性的属性值全部相同，则无法划分，此时将拥有样本数量最多的类别作为该结点的类别标记；
3. 当前结点包含的样本集合为空，不能划分，此时将该结点设为叶子结点，将父结点中所含样本数量最多的类别作为该叶子结点的类别。

****

## 2. 如何选择最优属性

### 2.1 信息增益==>ID3

### 2.2 增益率==>C4.5

### 2.3 基尼指数==>CART



